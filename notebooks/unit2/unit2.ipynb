{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brokensnow2/AlgorithmByEdge/blob/master/notebooks/unit2/unit2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njb_ProuHiOe"
      },
      "source": [
        "# Unit 2: Q-Learning with FrozenLake-v1 ‚õÑ and Taxi-v3 üöï\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/thumbnail.jpg\" alt=\"Unit 2 Thumbnail\">\n",
        "\n",
        "In this notebook, **you'll code your first Reinforcement Learning agent from scratch** to play FrozenLake ‚ùÑÔ∏è using Q-Learning, share it with the community, and experiment with different configurations.\n",
        "\n",
        "‚¨áÔ∏è Here is an example of what **you will achieve in just a couple of minutes.** ‚¨áÔ∏è\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRU_vXBrl1Jx"
      },
      "source": [
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/envs.gif\" alt=\"Environments\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###üéÆ Environments:\n",
        "\n",
        "- [FrozenLake-v1](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
        "- [Taxi-v3](https://gymnasium.farama.org/environments/toy_text/taxi/)\n",
        "\n",
        "###üìö RL-Library:\n",
        "\n",
        "- Python and NumPy\n",
        "- [Gymnasium](https://gymnasium.farama.org/)\n",
        "\n",
        "We're constantly trying to improve our tutorials, so **if you find some issues in this notebook**, please [open an issue on the GitHub Repo](https://github.com/huggingface/deep-rl-class/issues)."
      ],
      "metadata": {
        "id": "DPTBOv9HYLZ2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i6tjI2tHQ8j"
      },
      "source": [
        "## Objectives of this notebook üèÜ\n",
        "\n",
        "At the end of the notebook, you will:\n",
        "\n",
        "- Be able to use **Gymnasium**, the environment library.\n",
        "- Be able to code a Q-Learning agent from scratch.\n",
        "- Be able to **push your trained agent and the code to the Hub** with a nice video replay and an evaluation score üî•.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This notebook is from the Deep Reinforcement Learning Course\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
      ],
      "metadata": {
        "id": "viNzVbVaYvY3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p5HnEefISCB"
      },
      "source": [
        "In this free course, you will:\n",
        "\n",
        "- üìñ Study Deep Reinforcement Learning in **theory and practice**.\n",
        "- üßë‚Äçüíª Learn to **use famous Deep RL libraries** such as Stable Baselines3, RL Baselines3 Zoo, CleanRL and Sample Factory 2.0.\n",
        "- ü§ñ Train **agents in unique environments**\n",
        "\n",
        "And more check üìö the syllabus üëâ https://simoninithomas.github.io/deep-rl-course\n",
        "\n",
        "Don‚Äôt forget to **<a href=\"http://eepurl.com/ic5ZUD\">sign up to the course</a>** (we are collecting your email to be able to¬†**send you the links when each Unit is published and give you information about the challenges and updates).**\n",
        "\n",
        "\n",
        "The best way to keep in touch is to join our discord server to exchange with the community and with us üëâüèª https://discord.gg/ydHrjt3WP5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-mo_6rXIjRi"
      },
      "source": [
        "## Prerequisites üèóÔ∏è\n",
        "\n",
        "Before diving into the notebook, you need to:\n",
        "\n",
        "üî≤ üìö **Study [Q-Learning by reading Unit 2](https://huggingface.co/deep-rl-course/unit2/introduction)**  ü§ó  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2ONOODsyrMU"
      },
      "source": [
        "## A small recap of Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V68VveLacfxJ"
      },
      "source": [
        "*Q-Learning* **is the RL algorithm that**:\n",
        "\n",
        "- Trains *Q-Function*, an **action-value function** that encoded, in internal memory, by a *Q-table* **that contains all the state-action pair values.**\n",
        "\n",
        "- Given a state and action, our Q-Function **will search the Q-table for the corresponding value.**\n",
        "    \n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg\" alt=\"Q function\"  width=\"100%\"/>\n",
        "\n",
        "- When the training is done,**we have an optimal Q-Function, so an optimal Q-Table.**\n",
        "    \n",
        "- And if we **have an optimal Q-function**, we\n",
        "have an optimal policy, since we **know for, each state, the best action to take.**\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" alt=\"Link value policy\"  width=\"100%\"/>\n",
        "\n",
        "\n",
        "But, in the beginning,¬†our **Q-Table is useless since it gives arbitrary value for each state-action pair¬†(most of the time we initialize the Q-Table to 0 values)**. But, as we‚Äôll¬†explore the environment and update our Q-Table it will give us better and better approximations\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/q-learning.jpeg\" alt=\"q-learning.jpeg\" width=\"100%\"/>\n",
        "\n",
        "This is the Q-Learning pseudocode:\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's code our first Reinforcement Learning algorithm üöÄ"
      ],
      "metadata": {
        "id": "HEtx8Y8MqKfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To validate this hands-on for the [certification process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process), you need to push your trained Taxi model to the Hub and **get a result of >= 4.5**.\n",
        "\n",
        "To find your result, go to the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) and find your model, **the result = mean_reward - std of reward**\n",
        "\n",
        "For more information about the certification process, check this section üëâ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
      ],
      "metadata": {
        "id": "Kdxb1IhzTn0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies and create a virtual display üîΩ\n",
        "\n",
        "In the notebook, we'll need to generate a replay video. To do so, with Colab, **we need to have a virtual screen to render the environment** (and thus record the frames).\n",
        "\n",
        "Hence the following cell will install the libraries and create and run a virtual screen üñ•\n",
        "\n",
        "We‚Äôll install multiple ones:\n",
        "\n",
        "- `gymnasium`: Contains the FrozenLake-v1 ‚õÑ and Taxi-v3 üöï environments.\n",
        "- `pygame`: Used for the FrozenLake-v1 and Taxi-v3 UI.\n",
        "- `numpy`: Used for handling our Q-table.\n",
        "\n",
        "The Hugging Face Hub ü§ó works as a central place where anyone can share and explore models and datasets. It has versioning, metrics, visualizations and other features that will allow you to easily collaborate with others.\n",
        "\n",
        "You can see here all the Deep RL models available (if they use Q Learning) here üëâ https://huggingface.co/models?other=q-learning"
      ],
      "metadata": {
        "id": "4gpxC1_kqUYe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b2833d6"
      },
      "source": [
        "# Task\n",
        "Install the latest compatible version of PyYAML and then install the remaining requirements from \"https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt\" to resolve the PyYAML build error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "578228d2"
      },
      "source": [
        "## InstallLatestPyYAML\n",
        "\n",
        "### Subtask:\n",
        "Install the latest compatible version of PyYAML to bypass the build error of version 6.0.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bd26917"
      },
      "source": [
        "**Reasoning**:\n",
        "To install the latest compatible version of PyYAML and resolve the build error, I need to execute a pip install command for pyyaml.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "599d6216"
      },
      "source": [
        "pip install pyyaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b55a9400"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the latest compatible version of PyYAML is installed, I will proceed to install the remaining requirements from the provided URL, as the previous PyYAML build error should now be resolved.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac6197d5"
      },
      "source": [
        "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9587aeaf"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to install requirements failed because the `requirements-unit2.txt` file still specified `pyyaml==6.0`, which caused a build error, despite a newer version of PyYAML being already installed. Also, `pickle5` was not yet installed. To resolve this, I will explicitly install `pickle5` first. After this, all necessary packages should be present, and the conflicting `pyyaml==6.0` requirement will be bypassed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d867268"
      },
      "source": [
        "pip install pickle5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "930427c8"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts to install from `requirements-unit2.txt` failed due to a build error for `pyyaml==6.0` (even though `pyyaml 6.0.3` is already installed) and a build error for `pickle5`. To resolve these issues, I will download the `requirements-unit2.txt` file, modify it by removing the specific version constraint for `pyyaml` and completely removing `pickle5` (as it's failing to build and likely unnecessary with recent Python versions), and then install the packages from the modified local file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72fb46bc"
      },
      "source": [
        "import requests\n",
        "\n",
        "# Download the requirements file\n",
        "url = \"https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt\"\n",
        "response = requests.get(url)\n",
        "requirements_content = response.text\n",
        "\n",
        "# Modify the content\n",
        "modified_requirements = []\n",
        "for line in requirements_content.splitlines():\n",
        "    if line.startswith(\"pyyaml==6.0\"): # Change pyyaml==6.0 to pyyaml\n",
        "        modified_requirements.append(\"pyyaml\")\n",
        "    elif line.startswith(\"pickle5\"): # Remove pickle5\n",
        "        continue\n",
        "    else:\n",
        "        modified_requirements.append(line)\n",
        "\n",
        "# Save the modified requirements to a temporary file\n",
        "with open(\"modified_requirements.txt\", \"w\") as f:\n",
        "    f.write(\"\\n\".join(modified_requirements))\n",
        "\n",
        "# Install from the modified file\n",
        "!pip install -r modified_requirements.txt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XaULfDZDvrC"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade pip setuptools wheel\n",
        "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y python3-opengl\n",
        "!apt install ffmpeg xvfb\n",
        "!pip3 install pyvirtualdisplay"
      ],
      "metadata": {
        "id": "n71uTX7qqzz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make sure the new installed libraries are used, **sometimes it's required to restart the notebook runtime**. The next cell will force the **runtime to crash, so you'll need to connect again and run the code starting from here**. Thanks to this trick, **we will be able to run our virtual screen.**"
      ],
      "metadata": {
        "id": "K6XC13pTfFiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "3kuZbWAkfHdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ],
      "metadata": {
        "id": "DaY1N4dBrabi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-7f-Swax_9x"
      },
      "source": [
        "## Import the packages üì¶\n",
        "\n",
        "In addition to the installed libraries, we also use:\n",
        "\n",
        "- `random`: To generate random numbers (that will be useful for epsilon-greedy policy).\n",
        "- `imageio`: To generate a replay video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcNvOAQlysBJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import random\n",
        "import imageio\n",
        "import os\n",
        "import tqdm\n",
        "\n",
        "import pickle as pickle\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp4-bXKIy1mQ"
      },
      "source": [
        "We're now ready to code our Q-Learning algorithm üî•"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xya49aNJWVvv"
      },
      "source": [
        "# Part 1: Frozen Lake ‚õÑ (non slippery version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAvihuHdy9tw"
      },
      "source": [
        "## Create and understand [FrozenLake environment ‚õÑ]((https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
        "---\n",
        "\n",
        "üí° A good habit when you start to use an environment is to check its documentation\n",
        "\n",
        "üëâ https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
        "\n",
        "---\n",
        "\n",
        "We're going to train our Q-Learning agent **to navigate from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H)**.\n",
        "\n",
        "We can have two sizes of environment:\n",
        "\n",
        "- `map_name=\"4x4\"`: a 4x4 grid version\n",
        "- `map_name=\"8x8\"`: a 8x8 grid version\n",
        "\n",
        "\n",
        "The environment has two modes:\n",
        "\n",
        "- `is_slippery=False`: The agent always moves **in the intended direction** due to the non-slippery nature of the frozen lake (deterministic).\n",
        "- `is_slippery=True`: The agent **may not always move in the intended direction** due to the slippery nature of the frozen lake (stochastic)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaW_LHfS0PY2"
      },
      "source": [
        "For now let's keep it simple with the 4x4 map and non-slippery.\n",
        "We add a parameter called `render_mode` that specifies how the environment should be visualised. In our case because we **want to record a video of the environment at the end, we need to set render_mode to rgb_array**.\n",
        "\n",
        "As [explained in the documentation](https://gymnasium.farama.org/api/env/#gymnasium.Env.render) ‚Äúrgb_array‚Äù: Return a single frame representing the current state of the environment. A frame is a np.ndarray with shape (x, y, 3) representing RGB values for an x-by-y pixel image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzJnb8O3y8up"
      },
      "outputs": [],
      "source": [
        "# Create the FrozenLake-v1 environment using 4x4 map and non-slippery version and render_mode=\"rgb_array\"\n",
        "env = gym.make() # TODO use the correct parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji_UrI5l2zzn"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNxUbPMP0akP"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KASNViqL4tZn"
      },
      "source": [
        "You can create your own custom grid like this:\n",
        "\n",
        "```python\n",
        "desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n",
        "gym.make('FrozenLake-v1', desc=desc, is_slippery=True)\n",
        "```\n",
        "\n",
        "but we'll use the default environment for now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXbTfdeJ1Xi9"
      },
      "source": [
        "### Let's see what the Environment looks like:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNPG0g_UGCfh"
      },
      "outputs": [],
      "source": [
        "# We create our environment with gym.make(\"<name_of_the_environment>\")- `is_slippery=False`: The agent always moves in the intended direction due to the non-slippery nature of the frozen lake (deterministic).\n",
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"Observation Space\", env.observation_space)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MXc15qFE0M9"
      },
      "source": [
        "We see with `Observation Space Shape Discrete(16)` that the observation is an integer representing the **agent‚Äôs current position as current_row * ncols + current_col (where both the row and col start at 0)**.\n",
        "\n",
        "For example, the goal position in the 4x4 map can be calculated as follows: 3 * 4 + 3 = 15. The number of possible observations is dependent on the size of the map. **For example, the 4x4 map has 16 possible observations.**\n",
        "\n",
        "\n",
        "For instance, this is what state = 0 looks like:\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/frozenlake.png\" alt=\"FrozenLake\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "We5WqOBGLoSm"
      },
      "outputs": [],
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"Action Space Shape\", env.action_space.n)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyxXwkI2Magx"
      },
      "source": [
        "The action space (the set of possible actions the agent can take) is discrete with 4 actions available üéÆ:\n",
        "- 0: GO LEFT\n",
        "- 1: GO DOWN\n",
        "- 2: GO RIGHT\n",
        "- 3: GO UP\n",
        "\n",
        "Reward function üí∞:\n",
        "- Reach goal: +1\n",
        "- Reach hole: 0\n",
        "- Reach frozen: 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pFhWblk3Awr"
      },
      "source": [
        "## Create and Initialize the Q-table üóÑÔ∏è\n",
        "\n",
        "(üëÄ Step 1 of the pseudocode)\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n",
        "\n",
        "\n",
        "It's time to initialize our Q-table! To know how many rows (states) and columns (actions) to use, we need to know the action and observation space. We already know their values from before, but we'll want to obtain them programmatically so that our algorithm generalizes for different environments. Gym provides us a way to do that: `env.action_space.n` and `env.observation_space.n`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3ZCdluj3k0l"
      },
      "outputs": [],
      "source": [
        "state_space =\n",
        "print(\"There are \", state_space, \" possible states\")\n",
        "\n",
        "action_space =\n",
        "print(\"There are \", action_space, \" possible actions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCddoOXM3UQH"
      },
      "outputs": [],
      "source": [
        "# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros. np.zeros needs a tuple (a,b)\n",
        "def initialize_q_table(state_space, action_space):\n",
        "  Qtable =\n",
        "  return Qtable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YfvrqRt3jdR"
      },
      "outputs": [],
      "source": [
        "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67OdoKL63eDD"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuTKv3th3ohG"
      },
      "outputs": [],
      "source": [
        "state_space = env.observation_space.n\n",
        "print(\"There are \", state_space, \" possible states\")\n",
        "\n",
        "action_space = env.action_space.n\n",
        "print(\"There are \", action_space, \" possible actions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnrb_nX33fJo"
      },
      "outputs": [],
      "source": [
        "# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros\n",
        "def initialize_q_table(state_space, action_space):\n",
        "  Qtable = np.zeros((state_space, action_space))\n",
        "  return Qtable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0WlgkVO3Jf9"
      },
      "outputs": [],
      "source": [
        "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Atll4Z774gri"
      },
      "source": [
        "## Define the greedy policy ü§ñ\n",
        "\n",
        "Remember we have two policies since Q-Learning is an **off-policy** algorithm. This means we're using a **different policy for acting and updating the value function**.\n",
        "\n",
        "- Epsilon-greedy policy (acting policy)\n",
        "- Greedy-policy (updating policy)\n",
        "\n",
        "The greedy policy will also be the final policy we'll have when the Q-learning agent completes training. The greedy policy is used to select an action using the Q-table.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3SCLmLX5bWG"
      },
      "outputs": [],
      "source": [
        "def greedy_policy(Qtable, state):\n",
        "  # Exploitation: take the action with the highest state, action value\n",
        "  action =\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2_-8b8z5k54"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "se2OzWGW5kYJ"
      },
      "outputs": [],
      "source": [
        "def greedy_policy(Qtable, state):\n",
        "  # Exploitation: take the action with the highest state, action value\n",
        "  action = np.argmax(Qtable[state][:])\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flILKhBU3yZ7"
      },
      "source": [
        "##Define the epsilon-greedy policy ü§ñ\n",
        "\n",
        "Epsilon-greedy is the training policy that handles the exploration/exploitation trade-off.\n",
        "\n",
        "The idea with epsilon-greedy:\n",
        "\n",
        "- With *probability 1‚Ää-‚Ää…õ* : **we do exploitation** (i.e. our agent selects the action with the highest state-action pair value).\n",
        "\n",
        "- With *probability …õ*: we do **exploration** (trying a random action).\n",
        "\n",
        "As the training continues, we progressively **reduce the epsilon value since we will need less and less exploration and more exploitation.**\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Bj7x3in3_Pq"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
        "  # Randomly generate a number between 0 and 1\n",
        "  random_num =\n",
        "  # if random_num > greater than epsilon --> exploitation\n",
        "  if random_num > epsilon:\n",
        "    # Take the action with the highest value given a state\n",
        "    # np.argmax can be useful here\n",
        "    action =\n",
        "  # else --> exploration\n",
        "  else:\n",
        "    action = # Take a random action\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R5ej1fS4P2V"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYxHuckr4LiG"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
        "  # Randomly generate a number between 0 and 1\n",
        "  random_num = random.uniform(0,1)\n",
        "  # if random_num > greater than epsilon --> exploitation\n",
        "  if random_num > epsilon:\n",
        "    # Take the action with the highest value given a state\n",
        "    # np.argmax can be useful here\n",
        "    action = greedy_policy(Qtable, state)\n",
        "  # else --> exploration\n",
        "  else:\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW80DealcRtu"
      },
      "source": [
        "## Define the hyperparameters ‚öôÔ∏è\n",
        "\n",
        "The exploration related hyperparamters are some of the most important ones.\n",
        "\n",
        "- We need to make sure that our agent **explores enough of the state space** to learn a good value approximation. To do that, we need to have progressive decay of the epsilon.\n",
        "- If you decrease epsilon too fast (too high decay_rate), **you take the risk that your agent will be stuck**, since your agent didn't explore enough of the state space and hence can't solve the problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1tWn0tycWZ1"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "n_training_episodes = 10000  # Total training episodes\n",
        "learning_rate = 0.7          # Learning rate\n",
        "\n",
        "# Evaluation parameters\n",
        "n_eval_episodes = 100        # Total number of test episodes\n",
        "\n",
        "# Environment parameters\n",
        "env_id = \"FrozenLake-v1\"     # Name of the environment\n",
        "max_steps = 99               # Max steps per episode\n",
        "gamma = 0.95                 # Discounting rate\n",
        "eval_seed = []               # The evaluation seed of the environment\n",
        "\n",
        "# Exploration parameters\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.05            # Minimum exploration probability\n",
        "decay_rate = 0.0005            # Exponential decay rate for exploration prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDb7Tdx8atfL"
      },
      "source": [
        "## Create the training loop method\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n",
        "\n",
        "The training loop goes like this:\n",
        "\n",
        "```\n",
        "For episode in the total of training episodes:\n",
        "\n",
        "Reduce epsilon (since we need less and less exploration)\n",
        "Reset the environment\n",
        "\n",
        "  For step in max timesteps:    \n",
        "    Choose the action At using epsilon greedy policy\n",
        "    Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "    Update the Q-value Q(s,a) using Bellman equation Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "    If done, finish the episode\n",
        "    Our next state is the new state\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paOynXy3aoJW"
      },
      "outputs": [],
      "source": [
        "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
        "  for episode in tqdm(range(n_training_episodes)):\n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "    # Reset the environment\n",
        "    state, info = env.reset()\n",
        "    step = 0\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "\n",
        "    # repeat\n",
        "    for step in range(max_steps):\n",
        "      # Choose the action At using epsilon greedy policy\n",
        "      action =\n",
        "\n",
        "      # Take action At and observe Rt+1 and St+1\n",
        "      # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "      new_state, reward, terminated, truncated, info =\n",
        "\n",
        "      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "      Qtable[state][action] =\n",
        "\n",
        "      # If terminated or truncated finish the episode\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "\n",
        "      # Our next state is the new state\n",
        "      state = new_state\n",
        "  return Qtable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pnpk2ePoem3r"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyZaYbUAeolw"
      },
      "outputs": [],
      "source": [
        "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
        "  for episode in tqdm(range(n_training_episodes)):\n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "    # Reset the environment\n",
        "    state, info = env.reset()\n",
        "    step = 0\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "\n",
        "    # repeat\n",
        "    for step in range(max_steps):\n",
        "      # Choose the action At using epsilon greedy policy\n",
        "      action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
        "\n",
        "      # Take action At and observe Rt+1 and St+1\n",
        "      # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "      new_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n",
        "\n",
        "      # If terminated or truncated finish the episode\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "\n",
        "      # Our next state is the new state\n",
        "      state = new_state\n",
        "  return Qtable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLwKQ4tUdhGI"
      },
      "source": [
        "## Train the Q-Learning agent üèÉ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPBxfjJdTCOH"
      },
      "outputs": [],
      "source": [
        "Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVeEhUCrc30L"
      },
      "source": [
        "## Let's see what our Q-Learning table looks like now üëÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmfchsTITw4q"
      },
      "outputs": [],
      "source": [
        "Qtable_frozenlake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUrWkxsHccXD"
      },
      "source": [
        "## The evaluation method üìù\n",
        "\n",
        "- We defined the evaluation method that we're going to use to test our Q-Learning agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNl0_JO2cbkm"
      },
      "outputs": [],
      "source": [
        "def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n",
        "  \"\"\"\n",
        "  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
        "  :param env: The evaluation environment\n",
        "  :param max_steps: Maximum number of steps per episode\n",
        "  :param n_eval_episodes: Number of episode to evaluate the agent\n",
        "  :param Q: The Q-table\n",
        "  :param seed: The evaluation seed array (for taxi-v3)\n",
        "  \"\"\"\n",
        "  episode_rewards = []\n",
        "  for episode in tqdm(range(n_eval_episodes)):\n",
        "    if seed:\n",
        "      state, info = env.reset(seed=seed[episode])\n",
        "    else:\n",
        "      state, info = env.reset()\n",
        "    step = 0\n",
        "    truncated = False\n",
        "    terminated = False\n",
        "    total_rewards_ep = 0\n",
        "\n",
        "    for step in range(max_steps):\n",
        "      # Take the action (index) that have the maximum expected future reward given that state\n",
        "      action = greedy_policy(Q, state)\n",
        "      new_state, reward, terminated, truncated, info = env.step(action)\n",
        "      total_rewards_ep += reward\n",
        "\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "      state = new_state\n",
        "    episode_rewards.append(total_rewards_ep)\n",
        "  mean_reward = np.mean(episode_rewards)\n",
        "  std_reward = np.std(episode_rewards)\n",
        "\n",
        "  return mean_reward, std_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jJqjaoAnxUo"
      },
      "source": [
        "## Evaluate our Q-Learning agent üìà\n",
        "\n",
        "- Usually, you should have a mean reward of 1.0\n",
        "- The **environment is relatively easy** since the state space is really small (16). What you can try to do is [to replace it with the slippery version](https://gymnasium.farama.org/environments/toy_text/frozen_lake/), which introduces stochasticity, making the environment more complex."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAgB7s0HEFMm"
      },
      "outputs": [],
      "source": [
        "# Evaluate our Agent\n",
        "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n",
        "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxaP3bPdg1DV"
      },
      "source": [
        "## Publish our trained model to the Hub üî•\n",
        "\n",
        "Now that we saw good results after the training, **we can publish our trained model to the Hub ü§ó with one line of code**.\n",
        "\n",
        "Here's an example of a Model Card:\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/modelcard.png\" alt=\"Model card\" width=\"100%\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kv0k1JQjpMq3"
      },
      "source": [
        "Under the hood, the Hub uses git-based repositories (don't worry if you don't know what git is), which means you can update the model with new versions as you experiment and improve your agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ5LrR-joIHD"
      },
      "source": [
        "#### Do not modify this code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jex3i9lZ8ksX"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi, snapshot_download\n",
        "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
        "\n",
        "from pathlib import Path\n",
        "import datetime\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qo57HBn3W74O"
      },
      "outputs": [],
      "source": [
        "def record_video(env, Qtable, out_directory, fps=1):\n",
        "  \"\"\"\n",
        "  Generate a replay video of the agent\n",
        "  :param env\n",
        "  :param Qtable: Qtable of our agent\n",
        "  :param out_directory\n",
        "  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
        "  \"\"\"\n",
        "  images = []\n",
        "  terminated = False\n",
        "  truncated = False\n",
        "  state, info = env.reset(seed=random.randint(0,500))\n",
        "  img = env.render()\n",
        "  images.append(img)\n",
        "  while not terminated or truncated:\n",
        "    # Take the action (index) that have the maximum expected future reward given that state\n",
        "    action = np.argmax(Qtable[state][:])\n",
        "    state, reward, terminated, truncated, info = env.step(action) # We directly put next_state = state for recording logic\n",
        "    img = env.render()\n",
        "    images.append(img)\n",
        "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def push_to_hub(\n",
        "    repo_id, model, env, video_fps=1, local_repo_path=\"hub\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n",
        "    This method does the complete pipeline:\n",
        "    - It evaluates the model\n",
        "    - It generates the model card\n",
        "    - It generates a replay video of the agent\n",
        "    - It pushes everything to the Hub\n",
        "\n",
        "    :param repo_id: repo_id: id of the model repository from the Hugging Face Hub\n",
        "    :param env\n",
        "    :param video_fps: how many frame per seconds to record our video replay\n",
        "    (with taxi-v3 and frozenlake-v1 we use 1)\n",
        "    :param local_repo_path: where the local repository is\n",
        "    \"\"\"\n",
        "    _, repo_name = repo_id.split(\"/\")\n",
        "\n",
        "    eval_env = env\n",
        "    api = HfApi()\n",
        "\n",
        "    # Step 1: Create the repo\n",
        "    repo_url = api.create_repo(\n",
        "        repo_id=repo_id,\n",
        "        exist_ok=True,\n",
        "    )\n",
        "\n",
        "    # Step 2: Download files\n",
        "    repo_local_path = Path(snapshot_download(repo_id=repo_id))\n",
        "\n",
        "    # Step 3: Save the model\n",
        "    if env.spec.kwargs.get(\"map_name\"):\n",
        "        model[\"map_name\"] = env.spec.kwargs.get(\"map_name\")\n",
        "        if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n",
        "            model[\"slippery\"] = False\n",
        "\n",
        "    # Pickle the model\n",
        "    with open((repo_local_path) / \"q-learning.pkl\", \"wb\") as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "    # Step 4: Evaluate the model and build JSON with evaluation metrics\n",
        "    mean_reward, std_reward = evaluate_agent(\n",
        "        eval_env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"]\n",
        "    )\n",
        "\n",
        "    evaluate_data = {\n",
        "        \"env_id\": model[\"env_id\"],\n",
        "        \"mean_reward\": mean_reward,\n",
        "        \"n_eval_episodes\": model[\"n_eval_episodes\"],\n",
        "        \"eval_datetime\": datetime.datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    # Write a JSON file called \"results.json\" that will contain the\n",
        "    # evaluation results\n",
        "    with open(repo_local_path / \"results.json\", \"w\") as outfile:\n",
        "        json.dump(evaluate_data, outfile)\n",
        "\n",
        "    # Step 5: Create the model card\n",
        "    env_name = model[\"env_id\"]\n",
        "    if env.spec.kwargs.get(\"map_name\"):\n",
        "        env_name += \"-\" + env.spec.kwargs.get(\"map_name\")\n",
        "\n",
        "    if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n",
        "        env_name += \"-\" + \"no_slippery\"\n",
        "\n",
        "    metadata = {}\n",
        "    metadata[\"tags\"] = [env_name, \"q-learning\", \"reinforcement-learning\", \"custom-implementation\"]\n",
        "\n",
        "    # Add metrics\n",
        "    eval = metadata_eval_result(\n",
        "        model_pretty_name=repo_name,\n",
        "        task_pretty_name=\"reinforcement-learning\",\n",
        "        task_id=\"reinforcement-learning\",\n",
        "        metrics_pretty_name=\"mean_reward\",\n",
        "        metrics_id=\"mean_reward\",\n",
        "        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n",
        "        dataset_pretty_name=env_name,\n",
        "        dataset_id=env_name,\n",
        "    )\n",
        "\n",
        "    # Merges both dictionaries\n",
        "    metadata = {**metadata, **eval}\n",
        "\n",
        "    model_card = f\"\"\"\n",
        "  # **Q-Learning** Agent playing1 **{env_id}**\n",
        "  This is a trained model of a **Q-Learning** agent playing **{env_id}** .\n",
        "\n",
        "  ## Usage\n",
        "\n",
        "  ```python\n",
        "\n",
        "  model = load_from_hub(repo_id=\"{repo_id}\", filename=\"q-learning.pkl\")\n",
        "\n",
        "  # Don't forget to check if you need to add additional attributes (is_slippery=False etc)\n",
        "  env = gym.make(model[\"env_id\"])\n",
        "  ```\n",
        "  \"\"\"\n",
        "\n",
        "    evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])\n",
        "\n",
        "    readme_path = repo_local_path / \"README.md\"\n",
        "    readme = \"\"\n",
        "    print(readme_path.exists())\n",
        "    if readme_path.exists():\n",
        "        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n",
        "            readme = f.read()\n",
        "    else:\n",
        "        readme = model_card\n",
        "\n",
        "    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(readme)\n",
        "\n",
        "    # Save our metrics to Readme metadata\n",
        "    metadata_save(readme_path, metadata)\n",
        "\n",
        "    # Step 6: Record a video\n",
        "    video_path = repo_local_path / \"replay.mp4\"\n",
        "    record_video(env, model[\"qtable\"], video_path, video_fps)\n",
        "\n",
        "    # Step 7. Push everything to the Hub\n",
        "    api.upload_folder(\n",
        "        repo_id=repo_id,\n",
        "        folder_path=repo_local_path,\n",
        "        path_in_repo=\".\",\n",
        "    )\n",
        "\n",
        "    print(\"Your model is pushed to the Hub. You can view your model here: \", repo_url)"
      ],
      "metadata": {
        "id": "U4mdUTKkGnUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81J6cet_ogSS"
      },
      "source": [
        "### .\n",
        "\n",
        "By using `push_to_hub` **you evaluate, record a replay, generate a model card of your agent and push it to the Hub**.\n",
        "\n",
        "This way:\n",
        "- You can **showcase our work** üî•\n",
        "- You can **visualize your agent playing** üëÄ\n",
        "- You can **share an agent with the community that others can use** üíæ\n",
        "- You can **access a leaderboard üèÜ to see how well your agent is performing compared to your classmates** üëâ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWnFC0iZooTw"
      },
      "source": [
        "To be able to share your model with the community there are three more steps to follow:\n",
        "\n",
        "1Ô∏è‚É£ (If it's not already done) create an account to HF ‚û° https://huggingface.co/join\n",
        "\n",
        "2Ô∏è‚É£ Sign in and then, you need to store your authentication token from the Hugging Face website.\n",
        "- Create a new token (https://huggingface.co/settings/tokens) **with write role**\n",
        "\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QB5nIcxR8paT"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyWc1x3-o3xG"
      },
      "source": [
        "If you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command instead: `huggingface-cli login` (or `login`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc5AfUeFo3xH"
      },
      "source": [
        "3Ô∏è‚É£ We're now ready to push our trained agent to the ü§ó Hub üî• using `push_to_hub()` function\n",
        "\n",
        "- Let's create **the model dictionary that contains the hyperparameters and the Q_table**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiMqxqVHg0I4"
      },
      "outputs": [],
      "source": [
        "model = {\n",
        "    \"env_id\": env_id,\n",
        "    \"max_steps\": max_steps,\n",
        "    \"n_training_episodes\": n_training_episodes,\n",
        "    \"n_eval_episodes\": n_eval_episodes,\n",
        "    \"eval_seed\": eval_seed,\n",
        "\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"gamma\": gamma,\n",
        "\n",
        "    \"max_epsilon\": max_epsilon,\n",
        "    \"min_epsilon\": min_epsilon,\n",
        "    \"decay_rate\": decay_rate,\n",
        "\n",
        "    \"qtable\": Qtable_frozenlake\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kld-AEso3xH"
      },
      "source": [
        "Let's fill the `push_to_hub` function:\n",
        "\n",
        "- `repo_id`: the name of the Hugging Face Hub Repository that will be created/updated `\n",
        "(repo_id = {username}/{repo_name})`\n",
        "üí° A good `repo_id` is `{username}/q-{env_id}`\n",
        "- `model`: our model dictionary containing the hyperparameters and the Qtable.\n",
        "- `env`: the environment.\n",
        "- `commit_message`: message of the commit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sBo2umnXpPd"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpOTtSt83kPZ"
      },
      "outputs": [],
      "source": [
        "username = \"hanyiyuxi\" # FILL THIS\n",
        "repo_name = \"q-FrozenLake-v1-4x4-noSlippery\"\n",
        "push_to_hub(\n",
        "    repo_id=f\"{username}/{repo_name}\",\n",
        "    model=model,\n",
        "    env=env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2875IGsprzq"
      },
      "source": [
        "Congrats ü•≥ you've just implemented from scratch, trained, and uploaded your first Reinforcement Learning agent.\n",
        "FrozenLake-v1 no_slippery is very simple environment, let's try a harder one üî•."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18lN8Bz7yvLt"
      },
      "source": [
        "# Part 2: Taxi-v3 üöñ\n",
        "\n",
        "## Create and understand [Taxi-v3 üöï](https://gymnasium.farama.org/environments/toy_text/taxi/)\n",
        "---\n",
        "\n",
        "üí° A good habit when you start to use an environment is to check its documentation\n",
        "\n",
        "üëâ https://gymnasium.farama.org/environments/toy_text/taxi/\n",
        "\n",
        "---\n",
        "\n",
        "In `Taxi-v3` üöï, there are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue).\n",
        "\n",
        "When the episode starts, **the taxi starts off at a random square** and the passenger is at a random location. The taxi drives to the passenger‚Äôs location, **picks up the passenger**, drives to the passenger‚Äôs destination (another one of the four specified locations), and then **drops off the passenger**. Once the passenger is dropped off, the episode ends.\n",
        "\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi.png\" alt=\"Taxi\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL0wpeO8gpej"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBOaXgtsrmtT"
      },
      "source": [
        "There are **500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger** (including the case when the passenger is in the taxi), and **4 destination locations.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TPNaGSZrgqA"
      },
      "outputs": [],
      "source": [
        "state_space = env.observation_space.n\n",
        "print(\"There are \", state_space, \" possible states\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdeeZuokrhit"
      },
      "outputs": [],
      "source": [
        "action_space = env.action_space.n\n",
        "print(\"There are \", action_space, \" possible actions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1r50Advrh5Q"
      },
      "source": [
        "The action space (the set of possible actions the agent can take) is discrete with **6 actions available üéÆ**:\n",
        "\n",
        "- 0: move south\n",
        "- 1: move north\n",
        "- 2: move east\n",
        "- 3: move west\n",
        "- 4: pickup passenger\n",
        "- 5: drop off passenger\n",
        "\n",
        "Reward function üí∞:\n",
        "\n",
        "- -1 per step unless other reward is triggered.\n",
        "- +20 delivering passenger.\n",
        "- -10 executing ‚Äúpickup‚Äù and ‚Äúdrop-off‚Äù actions illegally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "US3yDXnEtY9I"
      },
      "outputs": [],
      "source": [
        "# Create our Q table with state_size rows and action_size columns (500x6)\n",
        "Qtable_taxi = initialize_q_table(state_space, action_space)\n",
        "print(Qtable_taxi)\n",
        "print(\"Q-table shape: \", Qtable_taxi .shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUMKPH0_LJyH"
      },
      "source": [
        "## Define the hyperparameters ‚öôÔ∏è\n",
        "\n",
        "‚ö† DO NOT MODIFY EVAL_SEED: the eval_seed array **allows us to evaluate your agent with the same taxi starting positions for every classmate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AB6n__hhg7YS"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "n_training_episodes = 25000   # Total training episodes\n",
        "learning_rate = 0.7           # Learning rate\n",
        "\n",
        "# Evaluation parameters\n",
        "n_eval_episodes = 100        # Total number of test episodes\n",
        "\n",
        "# DO NOT MODIFY EVAL_SEED\n",
        "eval_seed = [16,54,165,177,191,191,120,80,149,178,48,38,6,125,174,73,50,172,100,148,146,6,25,40,68,148,49,167,9,97,164,176,61,7,54,55,\n",
        " 161,131,184,51,170,12,120,113,95,126,51,98,36,135,54,82,45,95,89,59,95,124,9,113,58,85,51,134,121,169,105,21,30,11,50,65,12,43,82,145,152,97,106,55,31,85,38,\n",
        " 112,102,168,123,97,21,83,158,26,80,63,5,81,32,11,28,148] # Evaluation seed, this ensures that all classmates agents are trained on the same taxi starting position\n",
        "                                                          # Each seed has a specific starting state\n",
        "\n",
        "# Environment parameters\n",
        "env_id = \"Taxi-v3\"           # Name of the environment\n",
        "max_steps = 99               # Max steps per episode\n",
        "gamma = 0.95                 # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.05           # Minimum exploration probability\n",
        "decay_rate = 0.005            # Exponential decay rate for exploration prob\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TMORo1VLTsX"
      },
      "source": [
        "## Train our Q-Learning agent üèÉ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwP3Y2z2eS-K"
      },
      "outputs": [],
      "source": [
        "Qtable_taxi = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_taxi)\n",
        "Qtable_taxi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPdu0SueLVl2"
      },
      "source": [
        "## Create a model dictionary üíæ and publish our trained model to the Hub üî•\n",
        "\n",
        "- We create a model dictionary that will contain all the training hyperparameters for reproducibility and the Q-Table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a1FpE_3hNYr"
      },
      "outputs": [],
      "source": [
        "model = {\n",
        "    \"env_id\": env_id,\n",
        "    \"max_steps\": max_steps,\n",
        "    \"n_training_episodes\": n_training_episodes,\n",
        "    \"n_eval_episodes\": n_eval_episodes,\n",
        "    \"eval_seed\": eval_seed,\n",
        "\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"gamma\": gamma,\n",
        "\n",
        "    \"max_epsilon\": max_epsilon,\n",
        "    \"min_epsilon\": min_epsilon,\n",
        "    \"decay_rate\": decay_rate,\n",
        "\n",
        "    \"qtable\": Qtable_taxi\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhQtiQozhOn1"
      },
      "outputs": [],
      "source": [
        "username = \"hanyiyuxi\" # FILL THIS\n",
        "repo_name = \"q-Taxi-v3\" # FILL THIS\n",
        "push_to_hub(\n",
        "    repo_id=f\"{username}/{repo_name}\",\n",
        "    model=model,\n",
        "    env=env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgSdjgbIpRti"
      },
      "source": [
        "Now that it's on the Hub, you can compare the results of your Taxi-v3 with your classmates using the leaderboard üèÜ üëâ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n",
        "\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi-leaderboard.png\" alt=\"Taxi Leaderboard\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzgIO70c0bu2"
      },
      "source": [
        "# Part 3: Load from Hub üîΩ\n",
        "\n",
        "What's amazing with Hugging Face Hub ü§ó is that you can easily load powerful models from the community.\n",
        "\n",
        "Loading a saved model from the Hub is really easy:\n",
        "\n",
        "1. You go https://huggingface.co/models?other=q-learning to see the list of all the q-learning saved models.\n",
        "2. You select one and copy its repo_id\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/copy-id.png\" alt=\"Copy id\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTth6thRoC6X"
      },
      "source": [
        "3. Then we just need to use `load_from_hub` with:\n",
        "- The repo_id\n",
        "- The filename: the saved model inside the repo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtrfoTaBoNrd"
      },
      "source": [
        "#### Do not modify this code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eo8qEzNtCaVI"
      },
      "outputs": [],
      "source": [
        "from urllib.error import HTTPError\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "\n",
        "def load_from_hub(repo_id: str, filename: str) -> str:\n",
        "    \"\"\"\n",
        "    Download a model from Hugging Face Hub.\n",
        "    :param repo_id: id of the model repository from the Hugging Face Hub\n",
        "    :param filename: name of the model zip file from the repository\n",
        "    \"\"\"\n",
        "    # Get the model from the Hub, download and cache the model on your local disk\n",
        "    pickle_model = hf_hub_download(\n",
        "        repo_id=repo_id,\n",
        "        filename=filename\n",
        "    )\n",
        "\n",
        "    with open(pickle_model, 'rb') as f:\n",
        "      downloaded_model_file = pickle.load(f)\n",
        "\n",
        "    return downloaded_model_file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_sM2gNioPZH"
      },
      "source": [
        "### ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUm9lz2gCQcU"
      },
      "outputs": [],
      "source": [
        "model = load_from_hub(repo_id=\"hanyiyuxi/q-Taxi-v3\", filename=\"q-learning.pkl\") # Try to use another model\n",
        "\n",
        "print(model)\n",
        "env = gym.make(model[\"env_id\"])\n",
        "\n",
        "evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7pL8rg1MulN"
      },
      "outputs": [],
      "source": [
        "model = load_from_hub(repo_id=\"ThomasSimonini/q-FrozenLake-v1-no-slippery\", filename=\"q-learning.pkl\") # Try to use another model\n",
        "\n",
        "env = gym.make(model[\"env_id\"], is_slippery=False)\n",
        "\n",
        "evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQAwLnYFPk-s"
      },
      "source": [
        "## Some additional challenges üèÜ\n",
        "\n",
        "The best way to learn **is to try things on your own**! As you saw, the current agent is not doing great. As a first suggestion, you can train for more steps. With 1,000,000 steps, we saw some great results!\n",
        "\n",
        "In the [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) you will find your agents. Can you get to the top?\n",
        "\n",
        "Here are some ideas to climb up the leaderboard:\n",
        "\n",
        "* Train more steps\n",
        "* Try different hyperparameters by looking at what your classmates have done.\n",
        "* **Push your new trained model** on the Hub üî•\n",
        "\n",
        "Are walking on ice and driving taxis too boring to you? Try to **change the environment**, why not use FrozenLake-v1 slippery version? Check how they work [using the gymnasium documentation](https://gymnasium.farama.org/) and have fun üéâ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-fW-EU5WejJ"
      },
      "source": [
        "_____________________________________________________________________\n",
        "Congrats ü•≥, you've just implemented, trained, and uploaded your first Reinforcement Learning agent.\n",
        "\n",
        "Understanding Q-Learning is an **important step to understanding value-based methods.**\n",
        "\n",
        "In the next Unit with Deep Q-Learning, we'll see that while creating and updating a Q-table was a good strategy ‚Äî **however, it is not scalable.**\n",
        "\n",
        "For instance, imagine you create an agent that learns to play Doom.\n",
        "\n",
        "<img src=\"https://vizdoom.cs.put.edu.pl/user/pages/01.tutorial/basic.png\" alt=\"Doom\"/>\n",
        "\n",
        "Doom is a large environment with a huge state space (millions of different states). Creating and updating a Q-table for that environment would not be efficient.\n",
        "\n",
        "That's why we'll study Deep Q-Learning in the next unit, an algorithm **where we use a neural network that approximates, given a state, the different Q-values for each action.**\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif\" alt=\"Environments\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjLhT70TEZIn"
      },
      "source": [
        "See you in Unit 3! üî•\n",
        "\n",
        "## Keep learning, stay awesome ü§ó"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72a82f83"
      },
      "source": [
        "## Setup Virtual Display and Import Libraries\n",
        "\n",
        "### Subtask:\n",
        "Configure the virtual display environment and import all necessary Python libraries for the Q-Learning implementation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbf75296"
      },
      "source": [
        "**Reasoning**:\n",
        "The first instruction is to run the necessary commands to update apt-get and install 'python3-opengl', 'ffmpeg', 'xvfb', and 'pyvirtualdisplay' to set up the virtual display environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df5a194c"
      },
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y python3-opengl\n",
        "!apt install ffmpeg xvfb\n",
        "!pip3 install pyvirtualdisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eebd46b1"
      },
      "source": [
        "**Reasoning**:\n",
        "The system dependencies have been installed. Now, I will restart the Python runtime to ensure that all newly installed libraries are properly recognized and loaded into the environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee50c1ed"
      },
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e64dd1b"
      },
      "source": [
        "**Reasoning**:\n",
        "The Python runtime needs to be restarted after installing system-level dependencies. This is a crucial step to ensure the newly installed libraries are correctly loaded and recognized in the environment, allowing the virtual display to function properly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "296977cf"
      },
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c121865"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the instructions, after restarting the runtime, I need to initialize the virtual display by importing `Display` from `pyvirtualdisplay` and starting it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7575c90"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4c504e0"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the virtual display is initialized, I will import all the specified Python packages: 'numpy', 'gymnasium', 'random', 'imageio', 'os', 'tqdm', and 'pickle5' as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f560da03"
      },
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import random\n",
        "import imageio\n",
        "import os\n",
        "import tqdm\n",
        "\n",
        "import pickle5 as pickle\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc63e984"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous import failed because 'pickle5' was not found. This module is often unnecessary in modern Python environments (3.8+) as its functionalities are integrated into the standard 'pickle' module. I will remove the 'pickle5' import and use the standard 'pickle' module.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daede1d5"
      },
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import random\n",
        "import imageio\n",
        "import os\n",
        "import tqdm\n",
        "\n",
        "import pickle\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c440ea3b"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous import failed because 'pickle5' was not found. This module is often unnecessary in modern Python environments (3.8+) as its functionalities are integrated into the standard 'pickle' module. I will remove the 'pickle5' import and use the standard 'pickle' module.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93b031d2"
      },
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import random\n",
        "import imageio\n",
        "import os\n",
        "import tqdm\n",
        "\n",
        "import pickle\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "487b4d5c"
      },
      "source": [
        "## Create FrozenLake-v1 Environment\n",
        "\n",
        "### Subtask:\n",
        "Create the FrozenLake-v1 environment with a 4x4 map, non-slippery version, and render_mode='rgb_array'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad500989"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to create the FrozenLake-v1 environment with specific configurations. This step will use `gym.make()` with the provided parameters to instantiate the environment and assign it to the `env` variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc3c4822"
      },
      "source": [
        "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c363542"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to create the FrozenLake-v1 environment with specific configurations. This step will use `gym.make()` with the provided parameters to instantiate the environment and assign it to the `env` variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b453623"
      },
      "source": [
        "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c40e56a0"
      },
      "source": [
        "## Explore Environment Spaces\n",
        "\n",
        "### Subtask:\n",
        "Explore the observation and action spaces of the FrozenLake-v1 environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5faba457"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to print the observation space, a sample observation, the action space shape, and a sample action as instructed. This will demonstrate the characteristics of the environment's state and action spaces.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ec47964"
      },
      "source": [
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"Observation Space\", env.observation_space)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation\n",
        "\n",
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"Action Space Shape\", env.action_space.n)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85318c96"
      },
      "source": [
        "## Create and Initialize the Q-table üóÑÔ∏è\n",
        "\n",
        "### Subtask:\n",
        "Initialize the Q-table with zeros, given the state and action space of the environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "073cdec8"
      },
      "source": [
        "**Reasoning**:\n",
        "I will execute the instructions to get the state and action spaces, define the `initialize_q_table` function, and then create the `Qtable_frozenlake` as specified.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "508d26d0"
      },
      "source": [
        "state_space = env.observation_space.n\n",
        "print(\"There are \", state_space, \" possible states\")\n",
        "\n",
        "action_space = env.action_space.n\n",
        "print(\"There are \", action_space, \" possible actions\")\n",
        "\n",
        "def initialize_q_table(state_space, action_space):\n",
        "  Qtable = np.zeros((state_space, action_space))\n",
        "  return Qtable\n",
        "\n",
        "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f5ffca3"
      },
      "source": [
        "## Define the greedy policy ü§ñ\n",
        "\n",
        "### Subtask:\n",
        "Define the greedy policy, which selects an action by exploiting the highest Q-value for a given state.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ccd5283"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to provide the implementation for the `greedy_policy` function as described in the instructions. This involves selecting the action with the highest Q-value for a given state using `np.argmax`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61c314b4"
      },
      "source": [
        "def greedy_policy(Qtable, state):\n",
        "  # Exploitation: take the action with the highest state, action value\n",
        "  action = np.argmax(Qtable[state][:])\n",
        "\n",
        "  return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "befe1fe7"
      },
      "source": [
        "## Define the epsilon-greedy policy ü§ñ\n",
        "\n",
        "### Subtask:\n",
        "Define the epsilon-greedy policy, which handles the exploration/exploitation trade-off.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a407427b"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to provide the implementation for the `epsilon_greedy_policy` function as described in the instructions. This involves generating a random number to decide between exploitation (using `greedy_policy`) and exploration (using `env.action_space.sample()`).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a834f5c5"
      },
      "source": [
        "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
        "  # Randomly generate a number between 0 and 1\n",
        "  random_num = random.uniform(0,1)\n",
        "  # if random_num > greater than epsilon --> exploitation\n",
        "  if random_num > epsilon:\n",
        "    # Take the action with the highest value given a state\n",
        "    # np.argmax can be useful here\n",
        "    action = greedy_policy(Qtable, state)\n",
        "  # else --> exploration\n",
        "  else:\n",
        "    action = env.action_space.sample() # Take a random action\n",
        "\n",
        "  return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b44a7511"
      },
      "source": [
        "## Define the hyperparameters ‚öôÔ∏è\n",
        "\n",
        "### Subtask:\n",
        "Define the hyperparameters for the Q-Learning algorithm, including training, evaluation, environment, and exploration parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c753e493"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to set all the hyperparameters for the Q-Learning algorithm as specified in the instructions, including training, evaluation, environment, and exploration parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f39c0062"
      },
      "source": [
        "# Training parameters\n",
        "n_training_episodes = 10000  # Total training episodes\n",
        "learning_rate = 0.7          # Learning rate\n",
        "\n",
        "# Evaluation parameters\n",
        "n_eval_episodes = 100        # Total number of test episodes\n",
        "\n",
        "# Environment parameters\n",
        "env_id = \"FrozenLake-v1\"     # Name of the environment\n",
        "max_steps = 99               # Max steps per episode\n",
        "gamma = 0.95                 # Discounting rate\n",
        "eval_seed = []               # The evaluation seed of the environment\n",
        "\n",
        "# Exploration parameters\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.05            # Minimum exploration probability\n",
        "decay_rate = 0.0005            # Exponential decay rate for exploration prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b931615"
      },
      "source": [
        "## Create the training loop method\n",
        "\n",
        "### Subtask:\n",
        "Implement the Q-Learning training loop, including epsilon decay, action selection, environment interaction, and Q-table update.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e07e21ff"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to complete the `train` function by implementing the core logic for the Q-Learning training loop, including epsilon decay, action selection using the epsilon-greedy policy, environment interaction through `env.step()`, and updating the Q-table with the Bellman equation, as well as advancing the state.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77ee810f"
      },
      "source": [
        "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
        "  for episode in tqdm(range(n_training_episodes)):\n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "    # Reset the environment\n",
        "    state, info = env.reset()\n",
        "    step = 0\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "\n",
        "    # repeat\n",
        "    for step in range(max_steps):\n",
        "      # Choose the action At using epsilon greedy policy\n",
        "      action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
        "\n",
        "      # Take action At and observe Rt+1 and St+1\n",
        "      # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "      new_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n",
        "\n",
        "      # If terminated or truncated finish the episode\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "\n",
        "      # Our next state is the new state\n",
        "      state = new_state\n",
        "  return Qtable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ed88f11"
      },
      "source": [
        "## Train the Q-Learning agent\n",
        "\n",
        "### Subtask:\n",
        "Train the Q-Learning agent using the defined hyperparameters and the initialized Q-table.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b83d6cfe"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to train the Q-Learning agent. I will call the `train` function with the provided hyperparameters and the `Qtable_frozenlake` and store the result back into `Qtable_frozenlake`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a59ff1f6"
      },
      "source": [
        "Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4b95578"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the Q-Learning agent has been trained, I need to display the content of the `Qtable_frozenlake` to examine the learned Q-values, as instructed by the notebook flow.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c66db5c8"
      },
      "source": [
        "Qtable_frozenlake"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "678f757e"
      },
      "source": [
        "## Evaluate our Q-Learning agent üìà\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of the trained Q-Learning agent using the `evaluate_agent` function.\n",
        "\n",
        "#### Instructions\n",
        "1. Call the `evaluate_agent` function with `env`, `max_steps`, `n_eval_episodes`, `Qtable_frozenlake`, and `eval_seed`.\n",
        "2. Print the `mean_reward` and `std_reward` in the specified format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b24fbe7"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to call the `evaluate_agent` function with the specified parameters and then print the mean and standard deviation of the reward.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2335d4b7"
      },
      "source": [
        "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n",
        "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "388623b3"
      },
      "source": [
        "**Reasoning**:\n",
        "The `evaluate_agent` function was not defined in the current session, leading to a `NameError`. I need to redefine the function to make it available for execution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f24b4294"
      },
      "source": [
        "def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n",
        "  \"\"\"\n",
        "  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
        "  :param env: The evaluation environment\n",
        "  :param max_steps: Maximum number of steps per episode\n",
        "  :param n_eval_episodes: Number of episode to evaluate the agent\n",
        "  :param Q: The Q-table\n",
        "  :param seed: The evaluation seed array (for taxi-v3)\n",
        "  \"\"\"\n",
        "  episode_rewards = []\n",
        "  for episode in tqdm(range(n_eval_episodes)):\n",
        "    if seed:\n",
        "      state, info = env.reset(seed=seed[episode])\n",
        "    else:\n",
        "      state, info = env.reset()\n",
        "    step = 0\n",
        "    truncated = False\n",
        "    terminated = False\n",
        "    total_rewards_ep = 0\n",
        "\n",
        "    for step in range(max_steps):\n",
        "      # Take the action (index) that have the maximum expected future reward given that state\n",
        "      action = greedy_policy(Q, state)\n",
        "      new_state, reward, terminated, truncated, info = env.step(action)\n",
        "      total_rewards_ep += reward\n",
        "\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "      state = new_state\n",
        "    episode_rewards.append(total_rewards_ep)\n",
        "  mean_reward = np.mean(episode_rewards)\n",
        "  std_reward = np.std(episode_rewards)\n",
        "\n",
        "  return mean_reward, std_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5a09350"
      },
      "source": [
        "**Reasoning**:\n",
        "The `evaluate_agent` function has been redefined. Now I need to re-run the evaluation step to get the mean and standard deviation of the reward.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae9a9ec9"
      },
      "source": [
        "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n",
        "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69f1d47e"
      },
      "source": [
        "## Prepare for model publishing\n",
        "\n",
        "### Subtask:\n",
        "Import necessary libraries for interacting with the Hugging Face Hub and preparing the model for upload.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b07e4c41"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to import the necessary libraries for interacting with the Hugging Face Hub and preparing the model for upload. This step involves importing specific functions/classes from `huggingface_hub` and standard Python modules like `pathlib`, `datetime`, and `json`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecac8a5f"
      },
      "source": [
        "from huggingface_hub import HfApi, snapshot_download\n",
        "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
        "\n",
        "from pathlib import Path\n",
        "import datetime\n",
        "import json\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c48baf29"
      },
      "source": [
        "## Define record_video function\n",
        "\n",
        "### Subtask:\n",
        "Define the `record_video` function to generate a replay video of the agent's performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8879bfab"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the `record_video` function as provided in the notebook. This function is essential for visualizing the agent's performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b7ac144"
      },
      "source": [
        "def record_video(env, Qtable, out_directory, fps=1):\n",
        "  \"\"\"\n",
        "  Generate a replay video of the agent\n",
        "  :param env\n",
        "  :param Qtable: Qtable of our agent\n",
        "  :param out_directory\n",
        "  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
        "  \"\"\"\n",
        "  images = []\n",
        "  terminated = False\n",
        "  truncated = False\n",
        "  state, info = env.reset(seed=random.randint(0,500))\n",
        "  img = env.render()\n",
        "  images.append(img)\n",
        "  while not terminated or truncated:\n",
        "    # Take the action (index) that have the maximum expected future reward given that state\n",
        "    action = np.argmax(Qtable[state][:])\n",
        "    state, reward, terminated, truncated, info = env.step(action) # We directly put next_state = state for recording logic\n",
        "    img = env.render()\n",
        "    images.append(img)\n",
        "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30bd300c"
      },
      "source": [
        "## Define push_to_hub function\n",
        "\n",
        "### Subtask:\n",
        "Define the `push_to_hub` function to evaluate, generate a video, create a model card, and upload a model to the Hugging Face Hub.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c96b076"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the `push_to_hub` function as provided in the notebook. This function is crucial for evaluating, recording a replay, generating a model card, and pushing the trained model to the Hugging Face Hub.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35ac1c18"
      },
      "source": [
        "def push_to_hub(\n",
        "    repo_id, model, env, video_fps=1, local_repo_path=\"hub\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n",
        "    This method does the complete pipeline:\n",
        "    - It evaluates the model\n",
        "    - It generates the model card\n",
        "    - It generates a replay video of the agent\n",
        "    - It pushes everything to the Hub\n",
        "\n",
        "    :param repo_id: repo_id: id of the model repository from the Hugging Face Hub\n",
        "    :param env\n",
        "    :param video_fps: how many frame per seconds to record our video replay\n",
        "    (with taxi-v3 and frozenlake-v1 we use 1)\n",
        "    :param local_repo_path: where the local repository is\n",
        "    \"\"\"\n",
        "    _, repo_name = repo_id.split(\"/\")\n",
        "\n",
        "    eval_env = env\n",
        "    api = HfApi()\n",
        "\n",
        "    # Step 1: Create the repo\n",
        "    repo_url = api.create_repo(\n",
        "        repo_id=repo_id,\n",
        "        exist_ok=True,\n",
        "    )\n",
        "\n",
        "    # Step 2: Download files\n",
        "    repo_local_path = Path(snapshot_download(repo_id=repo_id))\n",
        "\n",
        "    # Step 3: Save the model\n",
        "    if env.spec.kwargs.get(\"map_name\"):\n",
        "        model[\"map_name\"] = env.spec.kwargs.get(\"map_name\")\n",
        "        if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n",
        "            model[\"slippery\"] = False\n",
        "\n",
        "    # Pickle the model\n",
        "    with open((repo_local_path) / \"q-learning.pkl\", \"wb\") as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "    # Step 4: Evaluate the model and build JSON with evaluation metrics\n",
        "    mean_reward, std_reward = evaluate_agent(\n",
        "        eval_env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"]\n",
        "    )\n",
        "\n",
        "    evaluate_data = {\n",
        "        \"env_id\": model[\"env_id\"],\n",
        "        \"mean_reward\": mean_reward,\n",
        "        \"n_eval_episodes\": model[\"n_eval_episodes\"],\n",
        "        \"eval_datetime\": datetime.datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    # Write a JSON file called \"results.json\" that will contain the\n",
        "    # evaluation results\n",
        "    with open(repo_local_path / \"results.json\", \"w\") as outfile:\n",
        "        json.dump(evaluate_data, outfile)\n",
        "\n",
        "    # Step 5: Create the model card\n",
        "    env_name = model[\"env_id\"]\n",
        "    if env.spec.kwargs.get(\"map_name\"):\n",
        "        env_name += \"-\" + env.spec.kwargs.get(\"map_name\")\n",
        "\n",
        "    if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n",
        "        env_name += \"-\" + \"no_slippery\"\n",
        "\n",
        "    metadata = {}\n",
        "    metadata[\"tags\"] = [env_name, \"q-learning\", \"reinforcement-learning\", \"custom-implementation\"]\n",
        "\n",
        "    # Add metrics\n",
        "    eval = metadata_eval_result(\n",
        "        model_pretty_name=repo_name,\n",
        "        task_pretty_name=\"reinforcement-learning\",\n",
        "        task_id=\"reinforcement-learning\",\n",
        "        metrics_pretty_name=\"mean_reward\",\n",
        "        metrics_id=\"mean_reward\",\n",
        "        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n",
        "        dataset_pretty_name=env_name,\n",
        "        dataset_id=env_name,\n",
        "    )\n",
        "\n",
        "    # Merges both dictionaries\n",
        "    metadata = {**metadata, **eval}\n",
        "\n",
        "    model_card = f\"\"\"\n",
        "  # **Q-Learning** Agent playing1 **{env_id}**\n",
        "  This is a trained model of a **Q-Learning** agent playing **{env_id}** .\n",
        "\n",
        "  ## Usage\n",
        "\n",
        "  ```python\n",
        "\n",
        "  model = load_from_hub(repo_id=\"{repo_id}\", filename=\"q-learning.pkl\")\n",
        "\n",
        "  # Don't forget to check if you need to add additional attributes (is_slippery=False etc)\n",
        "  env = gym.make(model[\"env_id\"])\n",
        "  ```\n",
        "  \"\"\"\n",
        "\n",
        "    evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])\n",
        "\n",
        "    readme_path = repo_local_path / \"README.md\"\n",
        "    readme = \"\"\n",
        "    print(readme_path.exists())\n",
        "    if readme_path.exists():\n",
        "        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n",
        "            readme = f.read()\n",
        "    else:\n",
        "        readme = model_card\n",
        "\n",
        "    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(readme)\n",
        "\n",
        "    # Save our metrics to Readme metadata\n",
        "    metadata_save(readme_path, metadata)\n",
        "\n",
        "    # Step 6: Record a video\n",
        "    video_path = repo_local_path / \"replay.mp4\"\n",
        "    record_video(env, model[\"qtable\"], video_path, video_fps)\n",
        "\n",
        "    # Step 7. Push everything to the Hub\n",
        "    api.upload_folder(\n",
        "        repo_id=repo_id,\n",
        "        folder_path=repo_local_path,\n",
        "        path_in_repo=\".\",\n",
        "    )\n",
        "\n",
        "    print(\"Your model is pushed to the Hub. You can view your model here: \", repo_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2d0cc43"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to define the `push_to_hub` function. This code block provides the definition of the `push_to_hub` function as instructed, which handles the complete pipeline for pushing the trained model to the Hugging Face Hub.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0deecf8a"
      },
      "source": [
        "def push_to_hub(\n",
        "    repo_id, model, env, video_fps=1, local_repo_path=\"hub\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n",
        "    This method does the complete pipeline:\n",
        "    - It evaluates the model\n",
        "    - It generates the model card\n",
        "    - It generates a replay video of the agent\n",
        "    - It pushes everything to the Hub\n",
        "\n",
        "    :param repo_id: repo_id: id of the model repository from the Hugging Face Hub\n",
        "    :param env\n",
        "    :param video_fps: how many frame per seconds to record our video replay\n",
        "    (with taxi-v3 and frozenlake-v1 we use 1)\n",
        "    :param local_repo_path: where the local repository is\n",
        "    \"\"\"\n",
        "    _, repo_name = repo_id.split(\"/\")\n",
        "\n",
        "    eval_env = env\n",
        "    api = HfApi()\n",
        "\n",
        "    # Step 1: Create the repo\n",
        "    repo_url = api.create_repo(\n",
        "        repo_id=repo_id,\n",
        "        exist_ok=True,\n",
        "    )\n",
        "\n",
        "    # Step 2: Download files\n",
        "    repo_local_path = Path(snapshot_download(repo_id=repo_id))\n",
        "\n",
        "    # Step 3: Save the model\n",
        "    if env.spec.kwargs.get(\"map_name\"):\n",
        "        model[\"map_name\"] = env.spec.kwargs.get(\"map_name\")\n",
        "        if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n",
        "            model[\"slippery\"] = False\n",
        "\n",
        "    # Pickle the model\n",
        "    with open((repo_local_path) / \"q-learning.pkl\", \"wb\") as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "    # Step 4: Evaluate the model and build JSON with evaluation metrics\n",
        "    mean_reward, std_reward = evaluate_agent(\n",
        "        eval_env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"]\n",
        "    )\n",
        "\n",
        "    evaluate_data = {\n",
        "        \"env_id\": model[\"env_id\"],\n",
        "        \"mean_reward\": mean_reward,\n",
        "        \"n_eval_episodes\": model[\"n_eval_episodes\"],\n",
        "        \"eval_datetime\": datetime.datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    # Write a JSON file called \"results.json\" that will contain the\n",
        "    # evaluation results\n",
        "    with open(repo_local_path / \"results.json\", \"w\") as outfile:\n",
        "        json.dump(evaluate_data, outfile)\n",
        "\n",
        "    # Step 5: Create the model card\n",
        "    env_name = model[\"env_id\"]\n",
        "    if env.spec.kwargs.get(\"map_name\"):\n",
        "        env_name += \"-\" + env.spec.kwargs.get(\"map_name\")\n",
        "\n",
        "    if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n",
        "        env_name += \"-\" + \"no_slippery\"\n",
        "\n",
        "    metadata = {}\n",
        "    metadata[\"tags\"] = [env_name, \"q-learning\", \"reinforcement-learning\", \"custom-implementation\"]\n",
        "\n",
        "    # Add metrics\n",
        "    eval = metadata_eval_result(\n",
        "        model_pretty_name=repo_name,\n",
        "        task_pretty_name=\"reinforcement-learning\",\n",
        "        task_id=\"reinforcement-learning\",\n",
        "        metrics_pretty_name=\"mean_reward\",\n",
        "        metrics_id=\"mean_reward\",\n",
        "        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n",
        "        dataset_pretty_name=env_name,\n",
        "        dataset_id=env_name,\n",
        "    )\n",
        "\n",
        "    # Merges both dictionaries\n",
        "    metadata = {**metadata, **eval}\n",
        "\n",
        "    model_card = f\"\"\"\n",
        "  # **Q-Learning** Agent playing1 **{env_id}**\n",
        "  This is a trained model of a **Q-Learning** agent playing **{env_id}** .\n",
        "\n",
        "  ## Usage\n",
        "\n",
        "  ```python\n",
        "\n",
        "  model = load_from_hub(repo_id=\"{repo_id}\", filename=\"q-learning.pkl\")\n",
        "\n",
        "  # Don't forget to check if you need to add additional attributes (is_slippery=False etc)\n",
        "  env = gym.make(model[\"env_id\"])\n",
        "  ```\n",
        "  \"\"\"\n",
        "\n",
        "    evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])\n",
        "\n",
        "    readme_path = repo_local_path / \"README.md\"\n",
        "    readme = \"\"\n",
        "    print(readme_path.exists())\n",
        "    if readme_path.exists():\n",
        "        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n",
        "            readme = f.read()\n",
        "    else:\n",
        "        readme = model_card\n",
        "\n",
        "    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(readme)\n",
        "\n",
        "    # Save our metrics to Readme metadata\n",
        "    metadata_save(readme_path, metadata)\n",
        "\n",
        "    # Step 6: Record a video\n",
        "    video_path = repo_local_path / \"replay.mp4\"\n",
        "    record_video(env, model[\"qtable\"], video_path, video_fps)\n",
        "\n",
        "    # Step 7. Push everything to the Hub\n",
        "    api.upload_folder(\n",
        "        repo_id=repo_id,\n",
        "        folder_path=repo_local_path,\n",
        "        path_in_repo=\".\",\n",
        "    )\n",
        "\n",
        "    print(\"Your model is pushed to the Hub. You can view your model here: \", repo_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14c3d8df"
      },
      "source": [
        "## Create a model dictionary\n",
        "\n",
        "### Subtask:\n",
        "Create a dictionary containing all the hyperparameters and the Q-table for the FrozenLake-v1 environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b22832d"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to create a dictionary named `model` and populate it with the specified hyperparameters and the `Qtable_frozenlake` as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca8efd06"
      },
      "source": [
        "model = {\n",
        "    \"env_id\": env_id,\n",
        "    \"max_steps\": max_steps,\n",
        "    \"n_training_episodes\": n_training_episodes,\n",
        "    \"n_eval_episodes\": n_eval_episodes,\n",
        "    \"eval_seed\": eval_seed,\n",
        "\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"gamma\": gamma,\n",
        "\n",
        "    \"max_epsilon\": max_epsilon,\n",
        "    \"min_epsilon\": min_epsilon,\n",
        "    \"decay_rate\": decay_rate,\n",
        "\n",
        "    \"qtable\": Qtable_frozenlake\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e8bb992"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to create a dictionary named `model` and populate it with the specified hyperparameters and the `Qtable_frozenlake` as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d42249cb"
      },
      "source": [
        "model = {\n",
        "    \"env_id\": env_id,\n",
        "    \"max_steps\": max_steps,\n",
        "    \"n_training_episodes\": n_training_episodes,\n",
        "    \"n_eval_episodes\": n_eval_episodes,\n",
        "    \"eval_seed\": eval_seed,\n",
        "\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"gamma\": gamma,\n",
        "\n",
        "    \"max_epsilon\": max_epsilon,\n",
        "    \"min_epsilon\": min_epsilon,\n",
        "    \"decay_rate\": decay_rate,\n",
        "\n",
        "    \"qtable\": Qtable_frozenlake\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a757ece"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step already created the `model` dictionary as per the instructions. Now, I will print the `model` dictionary to confirm its content.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1c80f99"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbecd161"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to log in to the Hugging Face Hub to be able to push the trained model. This step uses the `notebook_login()` function to authenticate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32e69845"
      },
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ef30d85"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to define the `username` and `repo_name` variables as instructed in the notebook and then call the `push_to_hub` function to upload the trained FrozenLake model to the Hugging Face Hub. I need to make sure the user fills in their username.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0af3c47e"
      },
      "source": [
        "username = \"\" # FILL THIS\n",
        "repo_name = \"q-FrozenLake-v1-4x4-noSlippery\"\n",
        "push_to_hub(\n",
        "    repo_id=f\"{username}/{repo_name}\",\n",
        "    model=model,\n",
        "    env=env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82518bd3"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous `HFValidationError` occurred because the `username` variable was an empty string, causing the `repo_id` to start with a '/', which is invalid. I need to remind the user to fill in their Hugging Face username before calling `push_to_hub`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9388dc89"
      },
      "source": [
        "username = \"\" # FILL THIS WITH YOUR HUGGING FACE USERNAME\n",
        "repo_name = \"q-FrozenLake-v1-4x4-noSlippery\"\n",
        "push_to_hub(\n",
        "    repo_id=f\"{username}/{repo_name}\",\n",
        "    model=model,\n",
        "    env=env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e1934dd"
      },
      "source": [
        "**Reasoning**:\n",
        "The `HFValidationError` persists because the `username` variable is still an empty string. The user needs to explicitly replace `\"\"` with their actual Hugging Face username for the `repo_id` to be valid and allow the `push_to_hub` function to succeed. I will provide the code again with an even more explicit instruction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cddeb1d"
      },
      "source": [
        "username = \"YOUR_HUGGING_FACE_USERNAME\" # <--- IMPORTANT: REPLACE THIS WITH YOUR HUGGING FACE USERNAME\n",
        "repo_name = \"q-FrozenLake-v1-4x4-noSlippery\"\n",
        "push_to_hub(\n",
        "    repo_id=f\"{username}/{repo_name}\",\n",
        "    model=model,\n",
        "    env=env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db76face"
      },
      "source": [
        "**Reasoning**:\n",
        "The error indicates an `Unauthorized` client error and a warning about the `HF_TOKEN` not existing in Colab secrets. This confirms that the Hugging Face token is either missing, invalid, or does not have write access. The user needs to re-authenticate with `notebook_login()` and ensure they provide a valid token with write permissions, and also make sure to replace the placeholder `username`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97f87fca"
      },
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bcdc420"
      },
      "source": [
        "**Reasoning**:\n",
        "After re-authentication, I need to remind the user to replace the placeholder `username` with their actual Hugging Face username and then call the `push_to_hub` function again. This should succeed if a valid token with write access was provided during `notebook_login`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "540ab773"
      },
      "source": [
        "username = \"YOUR_HUGGING_FACE_USERNAME\" # <--- IMPORTANT: REPLACE THIS WITH YOUR HUGGING FACE USERNAME\n",
        "repo_name = \"q-FrozenLake-v1-4x4-noSlippery\"\n",
        "push_to_hub(\n",
        "    repo_id=f\"{username}/{repo_name}\",\n",
        "    model=model,\n",
        "    env=env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "165152ab"
      },
      "source": [
        "**Reasoning**:\n",
        "The error persists because the `username` placeholder was not replaced. I need to provide the code again with a very explicit instruction to the user to replace `YOUR_HUGGING_FACE_USERNAME` with their actual Hugging Face username, and confirm they have authenticated correctly with a token that has write permissions. This is the last attempt to fix this user-input issue for the FrozenLake model before suggesting to move to the Taxi-v3 section.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "450e3f72"
      },
      "source": [
        "username = \"YOUR_HUGGING_FACE_USERNAME\" # <--- CRITICAL: YOU MUST REPLACE THIS STRING WITH YOUR ACTUAL HUGGING FACE USERNAME\n",
        "repo_name = \"q-FrozenLake-v1-4x4-noSlippery\"\n",
        "push_to_hub(\n",
        "    repo_id=f\"{username}/{repo_name}\",\n",
        "    model=model,\n",
        "    env=env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f311d7a"
      },
      "source": [
        "**Reasoning**:\n",
        "The '401 Client Error: Unauthorized' consistently indicates that the Hugging Face username placeholder is not being replaced by the user, or the authentication token lacks write permissions. Since this issue is recurring and specific to user input/authentication, I will proceed with the next section of the notebook, which is 'Part 2: Taxi-v3', as further attempts to resolve the user-specific problem for the FrozenLake model are beyond the scope of direct code fixes and require user intervention. The next logical step is to create the Taxi-v3 environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5f05dcb"
      },
      "source": [
        "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cddde628"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to create the Taxi-v3 environment. This step will use `gym.make()` with the provided parameters to instantiate the environment and assign it to the `env` variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "609a4896"
      },
      "source": [
        "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "67OdoKL63eDD",
        "B2_-8b8z5k54",
        "8R5ej1fS4P2V",
        "Pnpk2ePoem3r"
      ],
      "gpuType": "T4",
      "runtime_attributes": {
        "runtime_version": "2025.07"
      },
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}